{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Custom Stanford Alpaca Style Dataset for Model**\n",
        "\n",
        "\n",
        "1.   Project developed around Stanford Alpaca's project\n",
        "2.   Using OpenAI to generate dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "L1aOsngliGUg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GBJLfwkkcmhm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786922fe-cb55-4978-a5d2-eff4cdf0857a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "##Using GPT to generate more prompts\n",
        "!pip -q install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Working around Stanford's project\n",
        "!git clone https://github.com/tatsu-lab/stanford_alpaca.git\n",
        "%cd stanford_alpaca\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp6tI-qKiyBm",
        "outputId": "3298b489-c330-48e5-e0d8-44c3c0b4aeed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stanford_alpaca'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 129 (delta 57), reused 50 (delta 50), pack-reused 54\u001b[K\n",
            "Receiving objects: 100% (129/129), 9.14 MiB | 5.31 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "/content/stanford_alpaca\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.22.4)\n",
            "Collecting rouge_score (from -r requirements.txt (line 2))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fire (from -r requirements.txt (line 3))\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.27.8)\n",
            "Collecting transformers>=4.28.1 (from -r requirements.txt (line 5))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.0.1+cu118)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 7))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.3 (from -r requirements.txt (line 8))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from -r requirements.txt (line 9))\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score->-r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score->-r requirements.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai->-r requirements.txt (line 4)) (3.8.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.28.1->-r requirements.txt (line 5))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.28.1->-r requirements.txt (line 5)) (2022.10.31)\n",
            "Collecting safetensors>=0.3.1 (from transformers>=4.28.1->-r requirements.txt (line 5))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (16.0.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (8.1.4)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 9))\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 9))\n",
            "  Downloading sentry_sdk-1.28.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.2/213.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 9))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->-r requirements.txt (line 9))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->-r requirements.txt (line 9))\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (3.20.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.28.1->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai->-r requirements.txt (line 4)) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements.txt (line 4)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements.txt (line 4)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 6)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: rouge_score, fire, pathtools\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=19df45304c3044de37f0f0cbb0d984eb8f37a5eadfc83982703407467247a040\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=cacbe3d8ba8357e5417859dc8981a8d3161fb641437dc5d61121ccec456bb456\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=954174417fcbded476e283c0bb0cd5ffe8b642f9ae1c4e0efa1d0f7a335df929\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built rouge_score fire pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pathtools, smmap, setproctitle, sentry-sdk, fire, docker-pycreds, rouge_score, huggingface-hub, gitdb, transformers, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 fire-0.5.0 gitdb-4.0.10 huggingface-hub-0.16.4 pathtools-0.1.2 rouge_score-0.1.2 safetensors-0.3.1 sentencepiece-0.1.99 sentry-sdk-1.28.0 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformers-4.30.2 wandb-0.15.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "#Use private OpenAI API Key\n",
        "openai.api_key =''\n",
        "os.environ['OPENAI_API_KEY'] =''"
      ],
      "metadata": {
        "id": "zww8NxEEjP8J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Data generation process\n",
        "\"\"\"\n",
        "batch_selfinstruct_generate.py\n",
        "run:\n",
        "python -m generate_instruction generate_instruction_following_data \\\n",
        "  --output_dir ./ \\\n",
        "  --num_instructions_to_generate 10 \\\n",
        "  --model_name=\"text-davinci-003\" \\\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "import numpy as np\n",
        "import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "import utils\n",
        "import fire"
      ],
      "metadata": {
        "id": "SBU4SlODj3c0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Generation Process from Stanford Alpaca\n",
        "\n",
        "def encode_prompt(prompt_instructions):\n",
        "    \"\"\"Encode multiple prompt instructions into a single string.\"\"\"\n",
        "    prompt = open(\"./prompt.txt\").read() + \"\\n\"\n",
        "\n",
        "    for idx, task_dict in enumerate(prompt_instructions):\n",
        "        (instruction, input, output) = task_dict[\"instruction\"], task_dict[\"input\"], task_dict[\"output\"]\n",
        "        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n",
        "        input = \"<noinput>\" if input.lower() == \"\" else input\n",
        "        prompt += f\"###\\n\"\n",
        "        prompt += f\"{idx + 1}. Instruction: {instruction}\\n\"\n",
        "        prompt += f\"{idx + 1}. Input:\\n{input}\\n\"\n",
        "        prompt += f\"{idx + 1}. Output:\\n{output}\\n\"\n",
        "    prompt += f\"###\\n\"\n",
        "    prompt += f\"{idx + 2}. Instruction:\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def post_process_gpt3_response(num_prompt_instructions, response):\n",
        "    if response is None:\n",
        "        return []\n",
        "    raw_instructions = f\"{num_prompt_instructions+1}. Instruction:\" + response[\"text\"]\n",
        "    raw_instructions = re.split(\"###\", raw_instructions)\n",
        "    instructions = []\n",
        "    for idx, inst in enumerate(raw_instructions):\n",
        "        # if the decoding stops due to length, the last example is likely truncated so we discard it\n",
        "        if idx == len(raw_instructions) - 1 and response[\"finish_reason\"] == \"length\":\n",
        "            continue\n",
        "        idx += num_prompt_instructions + 1\n",
        "        splitted_data = re.split(f\"{idx}\\.\\s+(Instruction|Input|Output):\", inst)\n",
        "        if len(splitted_data) != 7:\n",
        "            continue\n",
        "        else:\n",
        "            inst = splitted_data[2].strip()\n",
        "            input = splitted_data[4].strip()\n",
        "            input = \"\" if input.lower() == \"<noinput>\" else input\n",
        "            output = splitted_data[6].strip()\n",
        "        # filter out too short or too long instructions\n",
        "        if len(inst.split()) <= 3 or len(inst.split()) > 150:\n",
        "            continue\n",
        "        # filter based on keywords that are not suitable for language models.\n",
        "        blacklist = [\n",
        "            \"image\",\n",
        "            \"images\",\n",
        "            \"graph\",\n",
        "            \"graphs\",\n",
        "            \"picture\",\n",
        "            \"pictures\",\n",
        "            \"file\",\n",
        "            \"files\",\n",
        "            \"map\",\n",
        "            \"maps\",\n",
        "            \"draw\",\n",
        "            \"plot\",\n",
        "            \"go to\",\n",
        "            \"video\",\n",
        "            \"audio\",\n",
        "            \"music\",\n",
        "            \"flowchart\",\n",
        "            \"diagram\",\n",
        "        ]\n",
        "        blacklist += []\n",
        "        if any(find_word_in_string(word, inst) for word in blacklist):\n",
        "            continue\n",
        "        # We found that the model tends to add \"write a program\" to some existing instructions, which lead to a lot of such instructions.\n",
        "        # And it's a bit comfusing whether the model need to write a program or directly output the result.\n",
        "        # Here we filter them out.\n",
        "        # Note this is not a comprehensive filtering for all programming instructions.\n",
        "        if inst.startswith(\"Write a program\"):\n",
        "            continue\n",
        "        # filter those starting with punctuation\n",
        "        if inst[0] in string.punctuation:\n",
        "            continue\n",
        "        # filter those starting with non-english character\n",
        "        if not inst[0].isascii():\n",
        "            continue\n",
        "        instructions.append({\"instruction\": inst, \"input\": input, \"output\": output})\n",
        "    return instructions\n",
        "\n",
        "\n",
        "def find_word_in_string(w, s):\n",
        "    return re.compile(r\"\\b({0})\\b\".format(w), flags=re.IGNORECASE).search(s)\n",
        "\n",
        "\n",
        "def generate_instruction_following_data(\n",
        "    output_dir=\"./\",\n",
        "    seed_tasks_path=\"./seed_tasks.jsonl\",\n",
        "    num_instructions_to_generate=100,\n",
        "    model_name=\"text-davinci-003\",\n",
        "    num_prompt_instructions=3,\n",
        "    request_batch_size=5,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    num_cpus=16,\n",
        "):\n",
        "    seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
        "    seed_instruction_data = [\n",
        "        {\"instruction\": t[\"instruction\"], \"input\": t[\"instances\"][0][\"input\"], \"output\": t[\"instances\"][0][\"output\"]}\n",
        "        for t in seed_tasks\n",
        "    ]\n",
        "    print(f\"Loaded {len(seed_instruction_data)} human-written seed instructions\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    request_idx = 0\n",
        "    # load the LM-generated instructions\n",
        "    machine_instruction_data = []\n",
        "    if os.path.exists(os.path.join(output_dir, \"regen.json\")):\n",
        "        machine_instruction_data = utils.jload(os.path.join(output_dir, \"regen.json\"))\n",
        "        print(f\"Loaded {len(machine_instruction_data)} machine-generated instructions\")\n",
        "\n",
        "    # similarities = {}\n",
        "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
        "\n",
        "    # now let's generate new instructions!\n",
        "    progress_bar = tqdm.tqdm(total=num_instructions_to_generate)\n",
        "    if machine_instruction_data:\n",
        "        progress_bar.update(len(machine_instruction_data))\n",
        "\n",
        "    # first we tokenize all the seed instructions and generated machine instructions\n",
        "    all_instructions = [d[\"instruction\"] for d in seed_instruction_data] + [\n",
        "        d[\"instruction\"] for d in machine_instruction_data\n",
        "    ]\n",
        "    all_instruction_tokens = [scorer._tokenizer.tokenize(inst) for inst in all_instructions]\n",
        "\n",
        "    while len(machine_instruction_data) < num_instructions_to_generate:\n",
        "        request_idx += 1\n",
        "\n",
        "        batch_inputs = []\n",
        "        for _ in range(request_batch_size):\n",
        "            # only sampling from the seed tasks\n",
        "            prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)\n",
        "            prompt = encode_prompt(prompt_instructions)\n",
        "            batch_inputs.append(prompt)\n",
        "        decoding_args = utils.OpenAIDecodingArguments(\n",
        "            temperature=temperature,\n",
        "            n=1,\n",
        "            max_tokens=3072,  # hard-code to maximize the length. the requests will be automatically adjusted\n",
        "            top_p=top_p,\n",
        "            stop=[\"\\n20\", \"20.\", \"20.\"],\n",
        "        )\n",
        "        request_start = time.time()\n",
        "        results = utils.openai_completion(\n",
        "            prompts=batch_inputs,\n",
        "            model_name=model_name,\n",
        "            batch_size=request_batch_size,\n",
        "            decoding_args=decoding_args,\n",
        "            logit_bias={\"50256\": -100},  # prevent the <|endoftext|> token from being generated\n",
        "        )\n",
        "        request_duration = time.time() - request_start\n",
        "\n",
        "        process_start = time.time()\n",
        "        instruction_data = []\n",
        "        for result in results:\n",
        "            new_instructions = post_process_gpt3_response(num_prompt_instructions, result)\n",
        "            instruction_data += new_instructions\n",
        "\n",
        "        total = len(instruction_data)\n",
        "        keep = 0\n",
        "        for instruction_data_entry in instruction_data:\n",
        "            # computing similarity with the pre-tokenzied instructions\n",
        "            new_instruction_tokens = scorer._tokenizer.tokenize(instruction_data_entry[\"instruction\"])\n",
        "            with Pool(num_cpus) as p:\n",
        "                rouge_scores = p.map(\n",
        "                    partial(rouge_scorer._score_lcs, new_instruction_tokens),\n",
        "                    all_instruction_tokens,\n",
        "                )\n",
        "            rouge_scores = [score.fmeasure for score in rouge_scores]\n",
        "            most_similar_instructions = {\n",
        "                all_instructions[i]: rouge_scores[i] for i in np.argsort(rouge_scores)[-10:][::-1]\n",
        "            }\n",
        "            if max(rouge_scores) > 0.7:\n",
        "                continue\n",
        "            else:\n",
        "                keep += 1\n",
        "            instruction_data_entry[\"most_similar_instructions\"] = most_similar_instructions\n",
        "            instruction_data_entry[\"avg_similarity_score\"] = float(np.mean(rouge_scores))\n",
        "            machine_instruction_data.append(instruction_data_entry)\n",
        "            all_instructions.append(instruction_data_entry[\"instruction\"])\n",
        "            all_instruction_tokens.append(new_instruction_tokens)\n",
        "            progress_bar.update(1)\n",
        "        process_duration = time.time() - process_start\n",
        "        print(f\"Request {request_idx} took {request_duration:.2f}s, processing took {process_duration:.2f}s\")\n",
        "        print(f\"Generated {total} instructions, kept {keep} instructions\")\n",
        "        utils.jdump(machine_instruction_data, os.path.join(output_dir, \"regen.json\"))\n"
      ],
      "metadata": {
        "id": "I-Msizr2keUT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Importing Data\n",
        "\n",
        "import json\n",
        "\n",
        "file_path = \"/content/stanford_alpaca/alpaca_data.json\"\n",
        "\n",
        "with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)"
      ],
      "metadata": {
        "id": "3Fs-YDDSkrwL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Seeding Tasks\n",
        "seed_tasks_path=\"./seed_tasks.jsonl\"\n",
        "\n",
        "seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
        "\n",
        "seed_instruction_data = [\n",
        "    {\"instruction\": t[\"instruction\"], \"input\": t[\"instances\"][0][\"input\"], \"output\": t[\"instances\"][0][\"output\"]}\n",
        "    for t in seed_tasks\n",
        "]\n",
        "print(f\"Loaded {len(seed_instruction_data)} human-written seed instructions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmkZBxSNkxZd",
        "outputId": "c0b28f9c-dc8f-443c-e8d0-1f8b5a8d26d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 175 human-written seed instructions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Creating a new (custom) set of seed tasks\n",
        "\n",
        "\n",
        "new_seed_tasks = seed_tasks[:3]\n",
        "new_seed_tasks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVQwittbk3m0",
        "outputId": "c4d690e2-7ba3-4a95-c6aa-71a30db411ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'seed_task_0',\n",
              "  'name': 'breakfast_suggestion',\n",
              "  'instruction': \"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\",\n",
              "  'instances': [{'input': '',\n",
              "    'output': 'Yes, you can have 1 oatmeal banana protein shake and 4 strips of bacon. The oatmeal banana protein shake may contain 1/2 cup oatmeal, 60 grams whey protein powder, 1/2 medium banana, 1tbsp flaxseed oil and 1/2 cup watter, totalling about 550 calories. The 4 strips of bacon contains about 200 calories.'}],\n",
              "  'is_classification': False},\n",
              " {'id': 'seed_task_1',\n",
              "  'name': 'antonym_relation',\n",
              "  'instruction': 'What is the relation between the given pairs?',\n",
              "  'instances': [{'input': 'Night : Day :: Right : Left',\n",
              "    'output': 'The relation between the given pairs is that they are opposites.'}],\n",
              "  'is_classification': False},\n",
              " {'id': 'seed_task_2',\n",
              "  'name': 'one_sentence_description',\n",
              "  'instruction': 'Generate a one-sentence description for each of the following people.',\n",
              "  'instances': [{'input': '- Brack Obama\\n- Elon Musk\\n- Taylor Swift',\n",
              "    'output': '- Barack Hussein Obama II is an American politician who served as the 44th president of the United States from 2009 to 2017.\\n- Elon Musk is the founder, CEO, and chief engineer of SpaceX; angel investor, CEO and product architect of Tesla, Inc.; founder of The Boring Company; co-founder of Neuralink and OpenAI; president of the Musk Foundation; and owner and CEO of Twitter, Inc.\\n- Taylor Alison Swift is an American singer-songwriter.'}],\n",
              "  'is_classification': False}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##New Data (relevant to company)\n",
        "json_new_tasks = '''\n",
        "[\n",
        "    {\n",
        "        \"id\": \"new_seed_task_3\",\n",
        "        \"name\": \"insurance_policies_offered\",\n",
        "        \"instruction\": \"What types of insurance policies do you offer?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"we offer a wide range of insurance policies, including life insurance, health insurance, motor insurance, travel insurance, and more. Our policies are designed to meet various coverage needs., tofu stir-fry, tempeh tacos, and quinoa stuffed peppers.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_4\",\n",
        "        \"name\": \"file_claim_insurance\",\n",
        "        \"instruction\": \"How can I file a claim for my car insurance?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"To file a claim for your car insurance, please reach out to our claims department at [contact information]. They will guide you through the claims process and assist you with the necessary documentation.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_5\",\n",
        "        \"name\": \"factors_premium_policy\",\n",
        "        \"instruction\": \"What factors affect the premium amount for a health insurance policy?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"Several factors impact the premium amount for a health insurance policy, including your age, medical history, coverage type, sum insured, and any additional riders you choose. These factors are taken into account while calculating your premium.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_6\",\n",
        "        \"name\": \"compare_insurance_plans\",\n",
        "        \"instruction\": \"Can you help me compare different life insurance plans?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"Absolutely! Our team of experts can assist you in comparing various life insurance plans based on factors like coverage, premium, benefits, and riders. Please provide us with some details, and we will guide you through the comparison process.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_7\",\n",
        "        \"name\": \"claim_settlement_ratio\",\n",
        "        \"instruction\": \"What is the claim settlement ratio of your company?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"We have a high claim settlement ratio, which indicates our commitment to efficiently settling valid claims. Our team strives to ensure a smooth and hassle-free claim process for our customers.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_8\",\n",
        "        \"name\": \"documents_motor_insurance\",\n",
        "        \"instruction\": \"What documents are required to purchase a motor insurance policy?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"To purchase a motor insurance policy, you will typically need documents such as vehicle registration certificate, driving license, address proof, and previous insurance details (if any). Our team can provide you with a detailed checklist based on your specific requirements.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_9\",\n",
        "        \"name\": \"great_experience\",\n",
        "        \"instruction\": \"I had a great experience with [Company]. The customer support was prompt and helpful. Thank you!\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"Thank you for your kind words! We're thrilled to hear that you had a positive experience with our customer support team. We strive to provide excellent service and are here to assist you whenever you need.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_10\",\n",
        "        \"name\": \"poor_experience\",\n",
        "        \"instruction\": \"I  am extremely dissatisfied with the claim process. It was confusing and time-consuming. I expected better service.\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"We apologize for the inconvenience you faced during the claim process. We understand your frustration, and we appreciate your feedback. We value your experience, and we'll use this feedback to improve our processes and ensure smoother claim handling in the future.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_11\",\n",
        "        \"name\": \"poor_feedback\",\n",
        "        \"instruction\": \"The premium for my car insurance policy increased significantly during the renewal. I wasn't informed about this in advance, and it came as a surprise. Disappointed with the lack of transparency.\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"We apologize for the inconvenience caused due to the sudden increase in the premium. We understand your concerns regarding transparency, and we appreciate your feedback. We'll review our communication processes to ensure that customers are informed about any changes in premiums well in advance.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_12\",\n",
        "        \"name\": \"life_insurance_policy\",\n",
        "        \"instruction\": \"Can you provide details about your term life insurance policy?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"Our term life insurance policy offers financial protection to your loved ones in the event of your untimely demise. It provides a lump sum payout to the nominee if the insured passes away during the policy term. The policy can be customized based on your coverage needs, and it offers affordable premiums.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_13\",\n",
        "        \"name\": \"health_insurance_policy\",\n",
        "        \"instruction\": \"What are the key features of your health insurance plans?\",\n",
        "        \"instances\": [{\"input\": \"Our health insurance plans provide coverage for medical expenses incurred due to illnesses, accidents, hospitalization, and more. Key features include cashless treatment at network hospitals, coverage for pre and post-hospitalization expenses, optional add-ons like critical illness cover, and access to a wide network of healthcare providers.\",\n",
        "                      \"output\": \"\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_14\",\n",
        "        \"name\": \"motor_insurance_policy\",\n",
        "        \"instruction\": \"Tell me about your motor insurance policies. What types of vehicles do you cover?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"Our motor insurance policies cover a range of vehicles, including cars, two-wheelers, commercial vehicles, and more. The policies provide protection against damages, theft, and third-party liabilities. We offer comprehensive coverage options tailored to suit your specific vehicle type and requirements.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_15\",\n",
        "        \"name\": \"travel_insurance_policy\",\n",
        "        \"instruction\": \"What benefits does your travel insurance policy offer?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"Our travel insurance policy provides coverage for various travel-related risks such as trip cancellation, medical emergencies, lost baggage, and personal liability. It offers assistance services during emergencies, 24/7 customer support, and coverage for both domestic and international travel.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_16\",\n",
        "        \"name\": \"riders_available\",\n",
        "        \"instruction\": \"Can you explain the different riders available with your life insurance policies?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"We offer several riders that can enhance the coverage of your life insurance policy. Some common riders include critical illness cover, accidental death benefit, waiver of premium, and income benefit rider. These riders provide additional financial protection and can be customized based on your needs.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_17\",\n",
        "        \"name\": \"interested_health_insurance\",\n",
        "        \"instruction\": \"I'm interested in purchasing a health insurance policy. Can you provide me with information about the coverage options?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"Of course! We offer a variety of health insurance coverage options tailored to meet your needs. Our policies provide coverage for hospitalization expenses, pre and post-hospitalization, day care procedures, and more. We can help you choose the right coverage based on your requirements and budget.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_18\",\n",
        "        \"name\": \"assistance_filing_claim\",\n",
        "        \"instruction\": \"I need assistance with filing a claim for my car insurance. What is the process?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"We're here to help! To file a claim for your car insurance, please gather all the necessary documentation, including the claim form, supporting evidence, and relevant invoices. You can then submit the claim through our online portal or contact our claims department directly. Our team will guide you through the process and assist you in resolving any queries.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_19\",\n",
        "        \"name\": \"change_policy_terms\",\n",
        "        \"instruction\": \"I received an email regarding a change in my policy terms. Can you clarify the details?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"Certainly! We understand your concern. Please forward us the email or provide us with the relevant details from the communication you received. Our team will review the changes and provide you with a detailed explanation to address any confusion or queries you may have.\"}],\n",
        "        \"is_classification\": false\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"new_seed_task_20\",\n",
        "        \"name\": \"update_contact_information\",\n",
        "        \"instruction\": \"I need to update my contact information on my insurance policy. How can I proceed?\",\n",
        "        \"instances\": [{\"input\": \"\",\n",
        "                      \"output\": \"We appreciate your request to update your contact information. To ensure a smooth process, please share your policy details and the updated contact information with us. You can do this by reaching out to our customer support team through our helpline or by visiting our website. We'll update your information promptly and confirm the changes.\"}],\n",
        "        \"is_classification\": false\n",
        "    }\n",
        "]\n",
        "'''"
      ],
      "metadata": {
        "id": "XAcspmLjlU1n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('new_seed_tasks.jsonl', 'w') as outfile:\n",
        "    for task_dict in new_seed_tasks:\n",
        "        json.dump(task_dict, outfile)\n",
        "        outfile.write('\\n')"
      ],
      "metadata": {
        "id": "kzQe7ODfshds"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Generation using OpenAI\n",
        "# Ensure to check plan with OpenAI to run the same\n",
        "!mkdir new_tasks\n",
        "!python -m generate_instruction generate_instruction_following_data --output_dir ./new_tasks/ --seed_tasks_path ./new_seed_tasks.jsonl --num_instructions_to_generate 1 --num_prompt_instructions 3 --request_batch_size 2 --num_cpus 4"
      ],
      "metadata": {
        "id": "3yrq66WnvfGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_instruction_following_data(\n",
        "    output_dir=\"./new_tasks/\",\n",
        "    seed_tasks_path=\"./new_seed_tasks.jsonl\",\n",
        "    num_instructions_to_generate=5,\n",
        "    model_name=\"text-davinci-003\",\n",
        "    num_prompt_instructions=3,\n",
        "    request_batch_size=2,\n",
        "    temperature=1.0,\n",
        "    top_p=1.0,\n",
        "    num_cpus=4,\n",
        ")"
      ],
      "metadata": {
        "id": "PsmNYeKuvinV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "file_path = \"/content/stanford_alpaca/new_tasks/regen.json\"\n",
        "\n",
        "with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# drop the most similar as measured by Rouge\n",
        "for dictionary in data:\n",
        "    dictionary.pop('most_similar_instructions', None)\n",
        "    dictionary.pop('avg_similarity_score',None)\n",
        "\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "rnMQnHFNu4Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credits Sam Witteveen"
      ],
      "metadata": {
        "id": "1mq46WGDvoFV"
      }
    }
  ]
}